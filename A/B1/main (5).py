import os
from collections import defaultdict

# ×¤×•× ×§×¦×™×” 1: ×—×œ×•×§×ª ×”×§×•×‘×¥ ×”×’×“×•×œ ×œ×§×‘×¦×™× ×§×˜× ×™× ×™×•×ª×¨
def divide_log_file(input_path, output_folder, lines_per_chunk):
    os.makedirs(output_folder, exist_ok=True)
    chunks = []
    current_chunk = []
    file_index = 0

    with open(input_path, 'r') as infile:
        for line_number, line in enumerate(infile, 1):
            current_chunk.append(line)

            if line_number % lines_per_chunk == 0:
                part_path = os.path.join(output_folder, f"log_part_{file_index}.txt")
                with open(part_path, 'w') as part_file:
                    part_file.writelines(current_chunk)
                chunks.append(part_path)
                current_chunk = []
                file_index += 1

        # ×©××™×¨×” ×©×œ ×”×—×œ×§ ×”××—×¨×•×Ÿ ×× × ×©××¨×• ×©×•×¨×•×ª
        if current_chunk:
            part_path = os.path.join(output_folder, f"log_part_{file_index}.txt")
            with open(part_path, 'w') as part_file:
                part_file.writelines(current_chunk)
            chunks.append(part_path)

    return chunks


# ×¤×•× ×§×¦×™×” 2: ×¡×¤×™×¨×ª ×§×•×“×™ ×©×’×™××” ××ª×•×š ×§×•×‘×¥ ×‘×•×“×“
def count_errors_from_file(file_path):
    counts = defaultdict(int)
    skipped_lines = 0

    with open(file_path, 'r') as f:
        for line in f:
            code = extract_error_code(line)
            if code:
                counts[code] += 1
            else:
                skipped_lines += 1

    return counts, skipped_lines


# ×¤×•× ×§×¦×™×” 3: ××™×–×•×’ ×¡×¤×™×¨×•×ª ×‘×™×Ÿ ××™×œ×•× ×™×
def combine_dictionaries(global_count, local_count):
    for key, value in local_count.items():
        global_count[key] += value


# ×¤×•× ×§×¦×™×” 4: ×—×™×œ×•×¥ ×§×•×“ ×©×’×™××” ××ª×•×š ×©×•×¨×ª ×˜×§×¡×˜
def extract_error_code(line):
    keyword = "Error: "
    start = line.find(keyword)

    if start == -1:
        return None

    start += len(keyword)
    end = line.find(' ', start)
    if end == -1:
        end = len(line)

    return line[start:end].strip()


# ----------------------------
# ×¤×•× ×§×¦×™×” 5: ××¦×™××ª N ×”×§×•×“×™× ×”×©×›×™×—×™× ×‘×™×•×ª×¨
# ----------------------------
def top_n_errors(error_dict, n):
    return sorted(error_dict.items(), key=lambda x: x[1], reverse=True)[:n]


# ×¤×•× ×§×¦×™×” ×¨××©×™×ª: ×”×ª×”×œ×™×š ×”××œ×
def process_log_file():
    input_path = r'C:\Users\user1\Desktop\Hadasim\part\logs1.txt'
    output_folder = r'C:\Users\user1\Desktop\log_parts\\'
    lines_per_file = 100000
    n = int(input("×›××” ×§×•×“×™ ×©×’×™××” ×œ×”×¦×™×’? "))

    print("\nğŸ”„ ××¤×¦×œ×ª ××ª ×”×§×•×‘×¥ ×”×’×“×•×œ...")
    chunk_files = divide_log_file(input_path, output_folder, lines_per_file)

    print("ğŸ“Š ×× ×ª×—×ª ×§×‘×¦×™×...")
    overall_counts = defaultdict(int)
    total_skipped = 0

    for chunk in chunk_files:
        local_counts, skipped = count_errors_from_file(chunk)
        combine_dictionaries(overall_counts, local_counts)
        total_skipped += skipped

    print("\nğŸ”¥ ×”×§×•×“×™× ×”×©×›×™×—×™× ×‘×™×•×ª×¨:")
    for code, count in top_n_errors(overall_counts, n):
        print(f"{code}: {count} ××•×¤×¢×™×")

    print(f"\nâ„¹ï¸ ×©×•×¨×•×ª ×©×œ× ×–×•×”×” ×‘×”×Ÿ ×§×•×“ ×©×’×™××”: {total_skipped}")


# × ×™×ª×•×— ×¡×™×‘×•×›×™×•×ª
# ×–××Ÿ:
#   ×§×¨×™××”: O(M) â€“ M = ××¡×¤×¨ ×©×•×¨×•×ª
#   ××™×•×Ÿ: O(E log E) â€“ E = ××¡×¤×¨ ×§×•×“×™ ×©×’×™××” ×©×•× ×™×
# ××§×•×:
#   ××—×¡×•×Ÿ ××™×œ×•×Ÿ ×§×•×“×™×: O(E)
#   ××—×¡×•×Ÿ ×–×× ×™ ×©×œ ×—×œ×§×™ ×§×‘×¦×™× â€“ ×¨×§ ×× × ×“×¨×©

# ×”×¤×¢×œ×ª ×”×ª×•×›× ×™×ª
if __name__ == "__main__":
    process_log_file()
